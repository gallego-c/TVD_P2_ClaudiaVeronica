{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b0R18qGjOxT"
   },
   "source": [
    "<center>\n",
    "<h1 style=\"font-family:verdana\">\n",
    " üíª üßë Reconeixement d'entitats anomenades üßë üíª\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZOmxM4_kAUC"
   },
   "source": [
    "<p> üéØ <b>Objectiu</b>: en aquesta segona part de la pr√†ctica aprendrem a recon√®ixer entitats anomenades, √©s a dir, identificar i classificar entitats en una oraci√≥ que poden ser (en el context de reserva de vols) la ciutat de sortida o d'arribada, la data del mes i el dia, si es de negocis o turista o altres categories espec√≠fiques. Aquesta tasca se sol plantejar etiquetant cada paraula amb una etiqueta de la categoria de l'entitat a la qual correspon.\n",
    "\n",
    "Tant la classificaci√≥ d'intencions (tasca estudiada a la primera part) com el reconeixement d'entitats anomenades (Name Entity Recognition, NER) que veurem en aquesta part s√≥n components crucials dels sistemes de processament del llenguatge natural (Natural Language Processing, NLP) i sovint s'utilitzen junts per crear aplicacions d'intel¬∑lig√®ncia artificial (IA) conversacionals m√©s sofisticades. Per exemple, en un xatbot, la classificaci√≥ d'intencions ajuda a entendre la intenci√≥ principal de l'usuari, mentre que el reconeixement d'entitats anomenades ajuda a extreure entitats rellevants per proporcionar respostes m√©s contextualitzades.\n",
    "\n",
    "\n",
    "<p> ‚ú® <b>Contingut</b>: els passos d'aquesta segona part seran similars a la primera. En primer lloc, analitzarem el conjunt de dades. En segon lloc, prepararem les dades per a poder entrenar el model. I finalment dissenyarem l'arquitectura i entrenarem el model. </p>\n",
    "\n",
    "\n",
    "<p> ‚úè <b>Exercicis</b>: en cada secci√≥ anireu trobant exercicis que haureu d'anar resolent. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI_K-e1cmXlb"
   },
   "source": [
    "---\n",
    "\n",
    "<h2> √çndex </h2>\n",
    "\n",
    "1. [Inspecci√≥ del conjunt de dades](#section-one)\n",
    "  * [Exercici 1](#ex-one)\n",
    "  * [Exercici 2](#ex-two)\n",
    "2. [Preprocessament de dades](#section-two)\n",
    "  * [Exercici 3](#ex-three)\n",
    "  * [Exercici 4](#ex-four)\n",
    "3. [Disseny del model i entrenament](#section-three)\n",
    "  * [Exercici 5](#ex-five)\n",
    "  * [Exercici 6](#ex-six)\n",
    "4. [Lliurable](#section-four)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 5815,
     "status": "ok",
     "timestamp": 1760536093103,
     "user": {
      "displayName": "Carlos Escolano",
      "userId": "02024451886670542795"
     },
     "user_tz": -120
    },
    "id": "XKn_1FnTxvfV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, LeakyReLU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Swhc9NwqpUs8"
   },
   "source": [
    "<h1><a name=\"section-one\"> 1. Inspecci√≥ del conjunt de dades </a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8688,
     "status": "ok",
     "timestamp": 1760536104885,
     "user": {
      "displayName": "Carlos Escolano",
      "userId": "02024451886670542795"
     },
     "user_tz": -120
    },
    "id": "SSM223cvp7wJ",
    "outputId": "4f8fcaf4-0559-4845-ed22-25fc4c6092d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.10.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u2wzXvsuscLeFHwXcDwMDaNDy0u_99-t\n",
      "To: /content/nlu_ATIS_data.tar.gz\n",
      "100% 122k/122k [00:00<00:00, 2.95MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install gdown\n",
    "# !gdown \"https://drive.google.com/uc?id=1u2wzXvsuscLeFHwXcDwMDaNDy0u_99-t\"\n",
    "# !tar -zxf nlu_ATIS_data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1760536111702,
     "user": {
      "displayName": "Carlos Escolano",
      "userId": "02024451886670542795"
     },
     "user_tz": -120
    },
    "id": "OAp0lemwjd71",
    "outputId": "c2dfb77b-52b9-4fdc-a2c3-b451f9d072e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"ls\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqKtLqYUpWkZ"
   },
   "source": [
    "En aquesta segona part de la pr√†ctica utilitzarem el mateix dataset que a la primera part, per√≤ en aquest cas utilitzarem la **primera columna** que correspon a les **oracions** introdu√Ødes pels usuaris i la **segona columna** on trobarem les oracions d'entrada en format **BILOU**.\n",
    "\n",
    "El format BILOU  √©s un esquema d'etiquetatge que es fa servir a les tasques de reconeixement d'entitats anomenades. El nom *BILOU* representa les etiquetes utilitzades en aquest esquema:\n",
    "\n",
    "*   **B** - Beginning: primer token d'una entitat.\n",
    "*   **I** - Inside: token dins d'una entitat.\n",
    "*   **L** - Last: √∫ltim token de l'entitat.\n",
    "*   **O** - Outside: token que no pertany a cap entitat.\n",
    "*   **U** - Unit: entitats d'un sol token\n",
    "\n",
    "A continuaci√≥, carregarem les dades per visualitzar i poder entendre millor aquestes etiquetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fUGDuD9wUfm"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-one\"><center> ‚úè Exercici 1 ‚úè</a></h1>\n",
    "\n",
    "A continuaci√≥ us demanem que carregueu els dos CSVs de la carpeta `data`: `train.csv`, `test.csv` utilitzant pandas. Recorda que aquests CSVs no tenen cap√ßalera. Agafeu les 900 √∫ltimes lineas del fitxer `train.csv` per crear un dataframe per validaci√≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "error",
     "timestamp": 1760536114938,
     "user": {
      "displayName": "Carlos Escolano",
      "userId": "02024451886670542795"
     },
     "user_tz": -120
    },
    "id": "36hvQo3GloN6",
    "outputId": "86c0633f-293b-4fd1-ef9b-261d25d5bbf9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i would like to find a flight from charlotte t...</td>\n",
       "      <td>\"O O O O O O O O B-fromloc.city_name O B-tolo...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on april first i need a ticket from tacoma to ...</td>\n",
       "      <td>\"O B-depart_date.month_name B-depart_date.day...</td>\n",
       "      <td>\"airfare\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on april first i need a flight going from phoe...</td>\n",
       "      <td>\"O B-depart_date.month_name B-depart_date.day...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i would like a flight traveling one way from p...</td>\n",
       "      <td>\"O O O O O O B-round_trip I-round_trip O B-fr...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i would like a flight from orlando to salt lak...</td>\n",
       "      <td>\"O O O O O O B-fromloc.city_name O B-toloc.ci...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>please find all the flights from cincinnati to...</td>\n",
       "      <td>\"O O O O O O B-fromloc.city_name O O O O O B-...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>find me a flight from cincinnati to any airpor...</td>\n",
       "      <td>\"O O O O O B-fromloc.city_name O O O O O B-to...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>i 'd like to fly from miami to chicago on amer...</td>\n",
       "      <td>\"O O O O O O B-fromloc.city_name O B-toloc.ci...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>i would like to book a round trip flight from ...</td>\n",
       "      <td>\"O O O O O O B-round_trip I-round_trip O O B-...</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>find me a flight that flies from memphis to ta...</td>\n",
       "      <td>\"O O O O O O O B-fromloc.city_name O B-toloc....</td>\n",
       "      <td>\"flight\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    i would like to find a flight from charlotte t...   \n",
       "1    on april first i need a ticket from tacoma to ...   \n",
       "2    on april first i need a flight going from phoe...   \n",
       "3    i would like a flight traveling one way from p...   \n",
       "4    i would like a flight from orlando to salt lak...   \n",
       "..                                                 ...   \n",
       "888  please find all the flights from cincinnati to...   \n",
       "889  find me a flight from cincinnati to any airpor...   \n",
       "890  i 'd like to fly from miami to chicago on amer...   \n",
       "891  i would like to book a round trip flight from ...   \n",
       "892  find me a flight that flies from memphis to ta...   \n",
       "\n",
       "                                                     1           2  \n",
       "0     \"O O O O O O O O B-fromloc.city_name O B-tolo...    \"flight\"  \n",
       "1     \"O B-depart_date.month_name B-depart_date.day...   \"airfare\"  \n",
       "2     \"O B-depart_date.month_name B-depart_date.day...    \"flight\"  \n",
       "3     \"O O O O O O B-round_trip I-round_trip O B-fr...    \"flight\"  \n",
       "4     \"O O O O O O B-fromloc.city_name O B-toloc.ci...    \"flight\"  \n",
       "..                                                 ...         ...  \n",
       "888   \"O O O O O O B-fromloc.city_name O O O O O B-...    \"flight\"  \n",
       "889   \"O O O O O B-fromloc.city_name O O O O O B-to...    \"flight\"  \n",
       "890   \"O O O O O O B-fromloc.city_name O B-toloc.ci...    \"flight\"  \n",
       "891   \"O O O O O O B-round_trip I-round_trip O O B-...    \"flight\"  \n",
       "892   \"O O O O O O O B-fromloc.city_name O B-toloc....    \"flight\"  \n",
       "\n",
       "[893 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1729339842515,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "93lI2dDVxmST",
    "outputId": "d0a460e8-21c5-4c45-cd82-3d5f0eb25fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4078\n",
      "Validation dataset size: 900\n",
      "Test dataset size: 893\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data/train.csv', header=None)\n",
    "val_data = train_data.tail(900)\n",
    "train_data = pd.read_csv('./data/train.csv', header=None, nrows=4078)\n",
    "test_data = pd.read_csv('./data/test.csv', header=None)\n",
    "\n",
    "print('Training size:', len(train_data))\n",
    "print('Validation dataset size:', len(val_data))\n",
    "print('Test dataset size:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CugYB4oNyFju"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-two\"><center> ‚úè Exercici 2 ‚úè</a></h1>\n",
    "\n",
    "Tal com hem introdu√Øt abans, per a aquest exercici ens centrarem en la **primera** i la **segona** columna. Per tant, ara us demanem que guardeu en les seg√ºents variables, les llistes corresponents a les oracions i a les etiquetes de les tres particions (`train`, `validation` i `test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1729339842515,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "edw3mK_Hy-M3",
    "outputId": "b57ed5d1-7219-4806-af3c-4629fbb5f5ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random number: 1834\n",
      "Sentence:  i need a flight from boston to pittsburgh\n",
      "Intent:   O O O O O B-fromloc.city_name O B-toloc.city_name\n"
     ]
    }
   ],
   "source": [
    "random_number = random.randint(0, len(train_data)-1)\n",
    "print('Random number:', random_number)\n",
    "\n",
    "train_sentences = list(train_data[0])\n",
    "train_labels = list(s.replace('\"', '') for s in train_data[1])\n",
    "# train_labels = list(s.replace(' ', '') for s in train_labels)\n",
    "\n",
    "val_sentences = list(val_data[0])\n",
    "val_labels = list(s.replace('\"', '') for s in val_data[1])\n",
    "# val_labels = list(s.replace(' ', '') for s in val_labels)\n",
    "\n",
    "test_sentences = list(test_data[0])\n",
    "test_labels = list(s.replace('\"', '') for s in test_data[1])\n",
    "# test_labels = list(s.replace(' ', '') for s in test_labels)\n",
    "\n",
    "print('Sentence: ', train_sentences[random_number])\n",
    "print('Intent: ', train_labels[random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKgiGo7UzYDU"
   },
   "source": [
    "---\n",
    "\n",
    "Si tot ha anat b√© ja podem analitzar quin aspecte t√© el format BILOU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1729339842515,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "lgjKAyCUzp_M",
    "outputId": "378a6225-456b-4e5f-d8fb-b14ababe1d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\n",
      " O O O O O B-fromloc.city_name O B-depart_time.time I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O B-arrive_time.period_of_day\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWzA_MPL02kk"
   },
   "source": [
    "Aquest hauria de ser el resultat obtingut si executeu la cel¬∑la anterior:\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\n",
    " \"O O O O O B-fromloc.city_name O B-depart_time.time I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O B-arrive_time.period_of_day\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "En aquest exemple, `boston` i `838 am` corresponen a l'entitat \"ciutat de sortida\" i a l'entitat \"hora de sortida\" respectivament (`fromloc.city_name`,`depart_time.time`). `838` correspon al primer token que pertany la entitat `depart_time.time` i `am` al segon de la mateixa entitat. Els tokens com `i` o `want` no pertanyen a cap entitat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729339842515,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "7BuHmIx62MUA",
    "outputId": "27503c1e-11e3-486f-a72b-ed784b186dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what flights are available from pittsburgh to baltimore on thursday morning\n",
      " O O O O O B-fromloc.city_name O B-toloc.city_name O B-depart_date.day_name B-depart_time.period_of_day\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[1])\n",
    "print(train_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSWlVauS2gll"
   },
   "source": [
    "I aquest hauria de ser el resultat de la cel¬∑la anterior:\n",
    "\n",
    "```\n",
    "what flights are available from pittsburgh to baltimore on thursday morning\n",
    " O O O O O B-fromloc.city_name O B-toloc.city_name O B-depart_date.day_name B-depart_time.period_of_day\n",
    "```\n",
    "\n",
    "En l'anterior exemple, `what`, `flights`, `are`, `available`, `from`, `to` i `on` estan etiquetades com a no pertanyents a cap entitat. I, en canvi, `pittsburgh` pertany a l'entitat d'un sol token anomenada ciutat d'arribada (`toloc.city_name`). Tamb√© hi ha entitats compostes com `baltimore` que pertany a l'entitat ciutat de sortida (`fromloc.city_name`) i `thursday morning` que pertanyen a les entitats data de sortida (`depart_date.day_name`, `depart_time.period_of_day`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hwZtqaI7rOQ"
   },
   "source": [
    "Vegem-ne la llista completa d'entitats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1729339842912,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "2VDzOwWk4Ee2",
    "outputId": "8ba5bb79-8345-4cab-fbf2-d1ecb46c0fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different entities: 119\n",
      "Unique entities: ['O', 'B-fromloc.city_name', 'B-depart_time.time', 'I-depart_time.time', 'B-toloc.city_name', 'B-arrive_time.time', 'B-arrive_time.period_of_day', 'B-depart_date.day_name', 'B-depart_time.period_of_day', 'B-flight_time', 'I-flight_time', 'I-fromloc.city_name', 'B-cost_relative', 'B-round_trip', 'I-round_trip', 'B-fare_amount', 'I-fare_amount', 'B-depart_date.today_relative', 'I-toloc.city_name', 'B-city_name', 'B-stoploc.city_name', 'B-toloc.airport_code', 'B-depart_time.time_relative', 'B-class_type', 'I-class_type', 'B-depart_date.date_relative', 'B-airline_name', 'I-airline_name', 'B-arrive_time.time_relative', 'B-depart_time.start_time', 'I-depart_time.start_time', 'B-depart_time.end_time', 'I-depart_time.end_time', 'B-fromloc.airport_name', 'I-fromloc.airport_name', 'B-toloc.state_name', 'B-depart_date.day_number', 'I-depart_date.day_number', 'B-depart_date.month_name', 'B-mod', 'B-fare_basis_code', 'B-transport_type', 'B-flight_mod', 'I-cost_relative', 'B-arrive_date.month_name', 'B-arrive_date.day_number', 'B-meal', 'B-toloc.state_code', 'B-meal_description', 'B-return_date.month_name', 'B-return_date.day_number', 'B-airline_code', 'B-depart_time.period_mod', 'I-arrive_time.time', 'B-flight_stop', 'I-city_name', 'B-fromloc.airport_code', 'B-arrive_date.day_name', 'B-time', 'B-or', 'I-arrive_date.day_number', 'B-economy', 'I-economy', 'B-flight_number', 'B-arrive_time.period_mod', 'I-transport_type', 'B-flight_days', 'I-stoploc.city_name', 'B-toloc.airport_name', 'I-toloc.airport_name', 'B-state_code', 'I-flight_stop', 'B-arrive_time.start_time', 'B-arrive_time.end_time', 'I-arrive_time.end_time', 'B-fromloc.state_name', 'I-fromloc.state_name', 'B-arrive_date.date_relative', 'B-depart_date.year', 'B-return_date.date_relative', 'B-airport_code', 'B-aircraft_code', 'B-fromloc.state_code', 'B-connect', 'I-arrive_time.start_time', 'B-restriction_code', 'I-restriction_code', 'I-toloc.state_name', 'B-airport_name', 'I-airport_name', 'B-toloc.country_name', 'B-days_code', 'I-fare_basis_code', 'I-arrive_time.time_relative', 'I-arrive_time.period_of_day', 'I-depart_time.time_relative', 'B-day_name', 'B-period_of_day', 'I-depart_date.day_name', 'B-today_relative', 'B-stoploc.state_code', 'B-meal_code', 'I-meal_code', 'I-today_relative', 'I-flight_mod', 'B-state_name', 'B-stoploc.airport_name', 'B-arrive_date.today_relative', 'B-time_relative', 'I-time', 'B-return_time.period_of_day', 'I-return_date.day_number', 'I-depart_time.period_of_day', 'B-return_time.period_mod', 'I-depart_date.today_relative', 'I-mod', 'B-month_name', 'B-day_number', 'I-return_date.date_relative']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_unique_entities(list_of_label_sentences):\n",
    "  flat_labels = []\n",
    "  for labels in list_of_label_sentences:\n",
    "    flat_labels += labels.split()\n",
    "  c = Counter(flat_labels)\n",
    "  return len(c), list(c.keys())\n",
    "\n",
    "num_unique_entities, unique_entities = count_unique_entities(train_labels)\n",
    "\n",
    "print(\"Number of different entities:\", num_unique_entities)\n",
    "\n",
    "print(\"Unique entities:\", unique_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wlsgR88_D8g"
   },
   "source": [
    "<h1><a name=\"section-two\"> 2. Preprocessament de dades </a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8yM79_n_OPA"
   },
   "source": [
    "El processament de les dades ser√† semblant al de la primera part d'aquesta pr√†ctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLrvW1frMus9"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-three\"><center> ‚úè Exercici 3 ‚úè</a></h1>\n",
    "\n",
    "En aquest exercici us demanem que realitzeu els passos seg√ºents per preparar les dades.\n",
    "\n",
    " 1. El primer pas ser√† construir el vocabulari a partir de les paraules presents a les oracions d'entrenament.\n",
    "\n",
    "2. El segon pas ser√† convertir les oracions en seq√º√®ncies de nombres enters usant el tokenitzador.\n",
    "\n",
    "3. El tercer pas ser√† guardar la longitud original de cada oraci√≥. Aix√≤ ens ser√† √∫til per evaluar el nostre model sense tenir en compte el padding.\n",
    "\n",
    "3. Finalment, per aconseguir que totes les seq√º√®ncies tinguen la mateixa longitud, fixarem la longitud segons la m√†xima trobada a l'entrenament i afegirem zeros a les oracions de menor longitud.\n",
    "\n",
    "Recordeu que podeu consultar com fer-ho a la documentaci√≥ de la llibrer√≠a:\n",
    "* <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\"> Tokenizer </a>\n",
    "* <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\"> Pad Sequences </a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "e0zOvP3UI_CE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'from': 2, 'flights': 3, 'the': 4, 'on': 5, 'what': 6, 'me': 7, 'flight': 8, 'boston': 9, 'show': 10, 'san': 11, 'i': 12, 'denver': 13, 'a': 14, 'francisco': 15, 'in': 16, 'and': 17, 'atlanta': 18, 'pittsburgh': 19, 'is': 20, 'dallas': 21, 'baltimore': 22, 'all': 23, 'philadelphia': 24, 'like': 25, 'are': 26, 'list': 27, 'airlines': 28, 'of': 29, 'between': 30, 'that': 31, 'washington': 32, 'leaving': 33, 'please': 34, 'pm': 35, 'morning': 36, 'would': 37, 'fly': 38, 'for': 39, 'fare': 40, 'first': 41, 'wednesday': 42, 'after': 43, 'there': 44, 'oakland': 45, \"'d\": 46, 'ground': 47, 'you': 48, 'does': 49, 'trip': 50, 'transportation': 51, 'class': 52, 'arriving': 53, 'cheapest': 54, 'need': 55, 'city': 56, 'round': 57, 'with': 58, 'before': 59, 'which': 60, 'available': 61, 'have': 62, 'give': 63, 'at': 64, 'fares': 65, 'american': 66, 'afternoon': 67, 'one': 68, 'want': 69, 'how': 70, 'way': 71, 'new': 72, 'dc': 73, 'nonstop': 74, 'arrive': 75, 'earliest': 76, 'york': 77, 'go': 78, 'monday': 79, 'thursday': 80, 'leave': 81, 'tuesday': 82, 'united': 83, 'am': 84, 'airport': 85, 'information': 86, 'st': 87, 'find': 88, 'evening': 89, 'sunday': 90, 'can': 91, 'milwaukee': 92, 'delta': 93, 'twenty': 94, 'miami': 95, 'noon': 96, 'las': 97, 'vegas': 98, 'charlotte': 99, 'chicago': 100, 'newark': 101, 'any': 102, 'phoenix': 103, \"o'clock\": 104, 'diego': 105, \"'s\": 106, 'august': 107, 'saturday': 108, 'continental': 109, 'do': 110, 'stop': 111, 'friday': 112, 'us': 113, 'air': 114, 'orlando': 115, 'next': 116, 'seventh': 117, 'july': 118, 'airline': 119, 'tell': 120, 'kansas': 121, 'seattle': 122, '5': 123, 'houston': 124, 'toronto': 125, 'or': 126, 'early': 127, 'indianapolis': 128, 'fort': 129, 'worth': 130, 'code': 131, 'aircraft': 132, 'latest': 133, 'cost': 134, 'tomorrow': 135, 'downtown': 136, 'cleveland': 137, 'los': 138, 'angeles': 139, 'many': 140, 'salt': 141, 'lake': 142, 'stopover': 143, 'around': 144, 'going': 145, '6': 146, 'by': 147, 'see': 148, '12': 149, '10': 150, 'an': 151, 'about': 152, 'dollars': 153, '8': 154, 'june': 155, 'may': 156, 'memphis': 157, 'montreal': 158, 'type': 159, 'twa': 160, 'petersburg': 161, 'get': 162, '7': 163, 'much': 164, 'jose': 165, 'leaves': 166, 'than': 167, 'minneapolis': 168, 'expensive': 169, \"'m\": 170, 'ticket': 171, 'tacoma': 172, 'could': 173, 'mean': 174, 'tampa': 175, 'nashville': 176, 'departing': 177, 'travel': 178, 'international': 179, 'time': 180, 'service': 181, 'long': 182, 'louis': 183, 'cincinnati': 184, 'less': 185, 'depart': 186, 'detroit': 187, 'meal': 188, 'into': 189, 'daily': 190, 'columbus': 191, 'know': 192, 'least': 193, 'okay': 194, 'economy': 195, 'it': 196, 'beach': 197, 'november': 198, 'coach': 199, 'last': 200, 'night': 201, 'paul': 202, 'book': 203, 'california': 204, 'used': 205, 'serves': 206, '4': 207, 'second': 208, 'day': 209, 'northwest': 210, 'return': 211, 'kind': 212, 'lowest': 213, 'december': 214, 'love': 215, 'field': 216, '1000': 217, 'serve': 218, 'make': 219, 'general': 220, 'mitchell': 221, 'now': 222, 'september': 223, 'schedule': 224, 'third': 225, 'be': 226, 'breakfast': 227, '2': 228, 'direct': 229, 'arrives': 230, 'stops': 231, '9': 232, 'flying': 233, 'number': 234, 'times': 235, 'goes': 236, 'stopping': 237, 'la': 238, 'looking': 239, 'possible': 240, 'burbank': 241, 'served': 242, 'dl': 243, 'as': 244, 'week': 245, 'airports': 246, 'take': 247, 'cities': 248, 'also': 249, 'eighth': 250, 'connecting': 251, 'this': 252, 'fifth': 253, 'fourth': 254, 'eastern': 255, '1991': 256, 'business': 257, 'ontario': 258, 'has': 259, 'most': 260, 'out': 261, 'car': 262, 'interested': 263, 'restriction': 264, 'ua': 265, 'april': 266, 'back': 267, 'two': 268, 'fifteenth': 269, 'same': 270, 'stand': 271, 'twentieth': 272, 'flies': 273, 'through': 274, 'late': 275, 'price': 276, 'ninth': 277, 'westchester': 278, 'via': 279, 'wednesdays': 280, 'prices': 281, 'using': 282, 'will': 283, 'airfare': 284, 'returning': 285, 'display': 286, 'plane': 287, 'ap': 288, '3': 289, 'arrangements': 290, 'your': 291, 'limousine': 292, 'sixth': 293, 'then': 294, 'only': 295, 'county': 296, 'listing': 297, 'bwi': 298, 'florida': 299, 'today': 300, 'dinner': 301, 'october': 302, 'other': 303, 'under': 304, 'types': 305, 'smallest': 306, 'eleventh': 307, 'tenth': 308, 'guardia': 309, 'thirtieth': 310, '1': 311, 'lunch': 312, 'tickets': 313, 'qx': 314, 'rental': 315, 'where': 316, 'canadian': 317, 'classes': 318, 'f': 319, 'jersey': 320, 'twelfth': 321, 'makes': 322, 'should': 323, 'airplane': 324, 'colorado': 325, 'either': 326, 'north': 327, 'carolina': 328, 'live': 329, 'h': 330, 'shortest': 331, 'midwest': 332, 'express': 333, 'capacity': 334, 'fourteenth': 335, 'meals': 336, 'again': 337, 'hi': 338, '57': 339, 'y': 340, 'but': 341, 'weekday': 342, 'people': 343, 'planes': 344, 'mco': 345, 'march': 346, 'no': 347, 'sixteenth': 348, 'nineteenth': 349, 'seating': 350, 'thirty': 351, 'seventeenth': 352, 'layover': 353, 'if': 354, 'yn': 355, 'my': 356, 'transport': 357, '11': 358, 'explain': 359, 'january': 360, 'when': 361, 'use': 362, 'traveling': 363, 'right': 364, 'some': 365, 'far': 366, 'both': 367, 'booking': 368, 'q': 369, 'serving': 370, 'thank': 371, 'during': 372, 'offer': 373, 'logan': 374, 'boeing': 375, 'sfo': 376, 'trying': 377, \"'re\": 378, '466': 379, 'each': 380, '1100': 381, 'abbreviation': 382, 'landings': 383, 'hp': 384, 'over': 385, 'february': 386, 'arrival': 387, 'numbers': 388, 'arrange': 389, 'coming': 390, 'distance': 391, 'midnight': 392, 'rent': 393, 'qw': 394, 'stopovers': 395, 'thrift': 396, 'ohio': 397, '530': 398, '630': 399, 'yes': 400, 'dfw': 401, 'canada': 402, 'let': 403, '281': 404, 'name': 405, 'takeoffs': 406, 'later': 407, 'uses': 408, 'lufthansa': 409, 'departure': 410, 'hours': 411, 'mornings': 412, 'codes': 413, 'sometime': 414, '838': 415, 'arrivals': 416, 'southwest': 417, 'area': 418, '230': 419, 'cheap': 420, '430': 421, 'nationair': 422, 'jfk': 423, 'days': 424, 'costs': 425, 'starting': 426, 'hello': 427, 'provided': 428, '1115': 429, '1245': 430, '1992': 431, 'ff': 432, 'total': 433, '747': 434, '718': 435, '2100': 436, 's': 437, 'making': 438, 'destination': 439, \"'ll\": 440, 'anywhere': 441, 'requesting': 442, 'kinds': 443, 'reservation': 444, 'america': 445, 'west': 446, 'sorry': 447, 'heading': 448, 'close': 449, 'f28': 450, 'minnesota': 451, 'their': 452, 'those': 453, 'seats': 454, 'pennsylvania': 455, 'saturdays': 456, 'sixteen': 457, 'eighteenth': 458, 'offers': 459, '80': 460, 'passengers': 461, 'fit': 462, 'rates': 463, 'sa': 464, 'land': 465, 'fn': 466, 'qo': 467, 'so': 468, '825': 469, 'help': 470, 'plan': 471, 'define': 472, 'tuesdays': 473, 'connect': 474, 'ea': 475, 'thursdays': 476, 'following': 477, 'more': 478, 'nw': 479, 'ewr': 480, 'noontime': 481, 'ten': 482, 'weekdays': 483, 'near': 484, 'dc10': 485, '934': 486, 'these': 487, 'include': 488, 'another': 489, 'departs': 490, '270': 491, '415': 492, '1110': 493, '755': 494, '720': 495, 'within': 496, 'c': 497, 'snack': 498, 'rate': 499, 'sure': 500, \"'t\": 501, 'rentals': 502, '100': 503, 'abbreviations': 504, '852': 505, 'approximately': 506, 'say': 507, 'wish': 508, 'difference': 509, '737': 510, 'highest': 511, 'connection': 512, '1700': 513, 'ord': 514, '813': 515, '2134': 516, 'maximum': 517, 'choices': 518, 'database': 519, '1765': 520, 'soon': 521, 'eight': 522, 'up': 523, 'departures': 524, 'quebec': 525, 'originate': 526, 'm80': 527, 'm': 528, '72s': 529, 'cp': 530, 'carries': 531, 'co': 532, 'here': 533, 'services': 534, 'taxi': 535, '555': 536, '201': 537, 'six': 538, 'connections': 539, 'philly': 540, 'located': 541, 'ap57': 542, 'dinnertime': 543, '1039': 544, 'lastest': 545, 'amount': 546, '21': 547, 'they': 548, 'just': 549, 'limo': 550, 'describe': 551, 'originating': 552, 'stapleton': 553, 'who': 554, '343': 555, 'options': 556, '1145': 557, 'schedules': 558, 'tennessee': 559, 'without': 560, 'landing': 561, 'midway': 562, '217': 563, 'bound': 564, 'different': 565, '296': 566, '324': 567, 'michigan': 568, 'train': 569, 'well': 570, 'along': 571, 'friends': 572, 'thirteenth': 573, 'transcontinental': 574, 'missouri': 575, 'utah': 576, 'cars': 577, 'dulles': 578, 'reservations': 579, 'lives': 580, '767': 581, '269': 582, 'turboprop': 583, 'sundays': 584, '757': 585, 'meaning': 586, 'taking': 587, 'proper': 588, 'beginning': 589, 'being': 590, '329': 591, '352': 592, 'don': 593, 'serviced': 594, '1024': 595, 'such': 596, 'wanted': 597, '615': 598, 'mealtime': 599, 'provides': 600, 'prefer': 601, '1288': 602, 'four': 603, '257': 604, 'across': 605, 'continent': 606, 'overnight': 607, 'trips': 608, 'local': 609, 'route': 610, '746': 611, 'represented': 612, 'trans': 613, 'world': 614, '1030': 615, '1130': 616, 'discount': 617, 'tower': 618, '2153': 619, 'thereafter': 620, '71': 621, 'supper': 622, 'bna': 623, '106': 624, 'd9s': 625, 'afterwards': 626, '345': 627, '19': 628, '82': 629, '139': 630, 'repeating': 631, '420': 632, 'look': 633, 'regarding': 634, 'nights': 635, 'seven': 636, 'restrictions': 637, '416': 638, 'kindly': 639, 'limousines': 640, 'place': 641, 'includes': 642, '1026': 643, '124': 644, 'fifteen': 645, 'oh': 646, 'year': 647, 'including': 648, 'o': 649, \"'hare\": 650, '815': 651, '928': 652, 'bur': 653, '315': 654, '1291': 655, 'longest': 656, '1222': 657, 'grounds': 658, '200': 659, 'must': 660, 'operation': 661, 'd': 662, '297': 663, 'question': 664, 'texas': 665, 'laying': 666, '650': 667, 'tonight': 668, '3724': 669, 'ls': 670, '210': 671, '1600': 672, 'inform': 673, 'k': 674, '932': 675, 'nonstops': 676, 'aa': 677, '459': 678, 'calling': 679, 'designate': 680, 'spend': 681, 'hou': 682, '1220': 683, 'directly': 684, 'reverse': 685, 'b': 686, 'belong': 687, '445': 688, '515': 689, '150': 690, '110': 691, 'connects': 692, 'charges': 693, 'minimum': 694, 'intercontinental': 695, '727': 696, 'takeoff': 697, 'seat': 698, 'final': 699, 'capacities': 700, '823': 701, '1059': 702, '271': 703, 'alaska': 704, 'able': 705, 'put': 706, 'locate': 707, 'hartfield': 708, 'scheduled': 709, '225': 710, '1158': 711, 'equipment': 712, 'begins': 713, 'lands': 714, 'reaches': 715, 'carried': 716, 'indiana': 717, 'try': 718, '130': 719, 'arizona': 720, 'continuing': 721, 'lester': 722, 'pearson': 723, 'everywhere': 724, '73s': 725, 'whether': 726, 'offered': 727, 'we': 728, 'usa': 729, 'red': 730, 'eye': 731, '1045': 732, 'georgia': 733, 'currently': 734, 'visit': 735, 'them': 736, '55': 737, 'determine': 738, 'thing': 739, '705': 740, 'fridays': 741, 'catch': 742, 'straight': 743, 'planning': 744, 'listed': 745, '1055': 746, '405': 747, 'equal': 748, 'ac': 749, 'provide': 750, 'hopefully': 751, 'symbols': 752, 'sort': 753, 'cover': 754, '810': 755, 'operating': 756, '1205': 757, 'mondays': 758, '733': 759, 'atl': 760, 'besides': 761, 'too': 762, 'month': 763, \"'ve\": 764, 'got': 765, 'somebody': 766, 'else': 767, 'wants': 768, 'level': 769, 'vicinity': 770, '311': 771, 'mia': 772, 'instead': 773, 'repeat': 774, 'eleven': 775, 'off': 776, 'greatest': 777, 'summer': 778, '300': 779, 'lax': 780, 'economic': 781, 'bay': 782, '402': 783, '845': 784, 'j31': 785, 'date': 786, '1020': 787, '730': 788, '400': 789, 'doesn': 790, '1993': 791, 'toward': 792, 'home': 793, '1850': 794, '1505': 795, 'runs': 796, 'stands': 797, '723': 798, 'thanks': 799, 'bring': 800, 'zone': 801, 'yyz': 802, 'airplanes': 803, 'non': 804, 'buy': 805, '500': 806, 'airfares': 807, 'come': 808, '428': 809, '98': 810, 'qualify': 811, '279': 812, '137338': 813, 'd10': 814, '539': 815, 'fine': 816, 'while': 817, '1200': 818, 'ap80': 819, 'concerning': 820, 'iah': 821, '1230': 822, 'still': 823, 'preferably': 824, 'itinerary': 825, '3357': 826, '323': 827, '229': 828, 'inexpensive': 829, 'actually': 830}\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "num_words=500\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "vocab = tokenizer.word_index\n",
    "print(vocab)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "len_train_sequences = [len(seq) for seq in train_sequences]\n",
    "max_sequence_length = max(len_train_sequences)\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "len_val_sequences = [len(seq) for seq in val_sequences]\n",
    "val_pad_sequences = pad_sequences(val_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "len_test_sequences = [len(seq) for seq in test_sequences]\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729339842912,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "UQPTc0CyOzLh",
    "outputId": "11200186-8274-4c8a-f948-9e119d71305b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  69,   1,  38,   2,   9,  64, 415,  84,  17,  75,  16,  13,\n",
       "        64, 493,  16,   4,  36,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n55MNuHfMegM"
   },
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wNo-X61OyRc"
   },
   "source": [
    "Tal com vam fer a la primera part, aqu√≠ tamb√© hem de convertir les diferents classes d'entitats en vectors one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD9_A1RJP1LZ"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-four\"><center> ‚úè Exercici 4 ‚úè</a></h1>\n",
    "\n",
    "Per aconseguir-ho haureu de seguir els passos seg√ºents.\n",
    "\n",
    " 1. En primer lloc, haureu d'esbrinar quantes etiquetes diferents hi ha. Podeu prendre com a exemple la funci√≥ `count_unique_entities` per fer-ho. Tingueu en compte que haureu de modificar la funci√≥, ja que per exemple aquesta funci√≥ considera que `B-depart_time.time`, `I-depart_time.time`, `L-depart_time.time`, `U-depart_time.time` s√≥n la mateixa entitat. En aquest exercici, necessitarem comptar-les per separat. Tamb√© l'entitat O ha de ser considerada com una classe.\n",
    "\n",
    " 2. El segon pas ser√† codificar les diferents classes trobades en etiquetes num√®riques. Tingueu en compte que cada paraula de l'oraci√≥ t√© una etiqueta i, per tant, per a cada oraci√≥ tindrem una llista d'etiquetes. El *padding* el codificarem amb l'etiqueta corresponent a **O** (outside). Per ajudar-vos hem preparat el processament per a les etiquetes d'entrenament, intenteu comprendre el que es fa i aix√≠ repetir-ho per a la partici√≥ de validaci√≥ i test.\n",
    "\n",
    " 3. Finalment, haureu de convertir les diferents classes a vectors one-hot. Recordeu de nou que per a cada oraci√≥ tindrem una llista de vectors one-hot.\n",
    "\n",
    "\n",
    " Podeu consultar els apartats de la documentaci√≥:\n",
    " * <a href=https://www.tensorflow.org/guide/keras/understanding_masking_and_padding> Masking and Padding </a>\n",
    "\n",
    " * <a href=https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical> To Categorical </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 3003,
     "status": "error",
     "timestamp": 1729339845912,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "Xl8GyFz4MgY3",
    "outputId": "534922cc-2c49-4b35-a3e2-c6c387128a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different entities: 120\n",
      "Unique entities: ['<pad>', 'B-toloc.state_name', 'B-depart_date.date_relative', 'I-flight_stop', 'B-arrive_date.day_number', 'I-arrive_time.period_of_day', 'B-cost_relative', 'B-arrive_date.today_relative', 'O', 'B-stoploc.airport_name', 'I-fromloc.state_name', 'B-transport_type', 'I-toloc.airport_name', 'B-flight_number', 'B-city_name', 'I-economy', 'B-flight_mod', 'B-depart_date.day_number', 'B-day_name', 'I-city_name', 'I-meal_code', 'B-fare_amount', 'B-depart_time.time', 'B-arrive_time.start_time', 'I-mod', 'I-return_date.date_relative', 'B-fromloc.state_name', 'B-depart_time.start_time', 'I-airline_name', 'B-toloc.airport_code', 'B-arrive_date.day_name', 'B-state_code', 'B-state_name', 'B-round_trip', 'B-days_code', 'I-transport_type', 'B-return_date.date_relative', 'B-time_relative', 'B-stoploc.city_name', 'B-airline_code', 'B-today_relative', 'B-airport_code', 'B-arrive_time.time', 'B-or', 'B-arrive_time.time_relative', 'B-depart_date.month_name', 'I-fare_amount', 'I-round_trip', 'B-airline_name', 'I-arrive_time.start_time', 'B-arrive_date.date_relative', 'B-meal', 'B-arrive_time.end_time', 'B-fromloc.state_code', 'B-fromloc.airport_name', 'I-fromloc.airport_name', 'B-restriction_code', 'B-depart_time.period_of_day', 'B-depart_date.today_relative', 'B-depart_time.time_relative', 'B-arrive_time.period_of_day', 'I-depart_time.start_time', 'B-class_type', 'B-toloc.city_name', 'B-return_date.day_number', 'B-meal_code', 'I-airport_name', 'I-depart_date.day_name', 'B-period_of_day', 'B-flight_stop', 'I-depart_date.day_number', 'B-stoploc.state_code', 'B-airport_name', 'I-flight_mod', 'B-month_name', 'I-restriction_code', 'B-time', 'I-toloc.city_name', 'B-depart_time.end_time', 'B-depart_time.period_mod', 'I-toloc.state_name', 'B-return_time.period_mod', 'B-fromloc.airport_code', 'I-depart_date.today_relative', 'I-arrive_time.end_time', 'B-toloc.state_code', 'B-fromloc.city_name', 'I-today_relative', 'B-connect', 'I-return_date.day_number', 'B-arrive_date.month_name', 'I-arrive_time.time', 'I-arrive_date.day_number', 'B-return_date.month_name', 'B-mod', 'I-depart_time.end_time', 'B-meal_description', 'I-depart_time.period_of_day', 'I-fare_basis_code', 'I-time', 'B-fare_basis_code', 'B-depart_date.year', 'B-flight_days', 'B-toloc.country_name', 'I-stoploc.city_name', 'B-day_number', 'I-depart_time.time_relative', 'I-class_type', 'B-return_time.period_of_day', 'I-fromloc.city_name', 'B-toloc.airport_name', 'I-arrive_time.time_relative', 'I-depart_time.time', 'I-flight_time', 'B-arrive_time.period_mod', 'B-flight_time', 'B-economy', 'I-cost_relative', 'B-aircraft_code', 'B-depart_date.day_name']\n",
      "[119 119 119 119 119  46 119  34  95 119 119 119  73 119  15 119 119  13\n",
      " 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119\n",
      " 119 119 119 119 119 119 119 119 119]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Step 1\n",
    "def count_unique_entities(list_of_label_sentences):\n",
    "    unique_entities = set()\n",
    "    for sentence_labels in list_of_label_sentences:\n",
    "        entity_labels = sentence_labels.split()\n",
    "        for label in entity_labels:\n",
    "            unique_entities.add(label)\n",
    "    unique_entities = ['<pad>']+(list(unique_entities))\n",
    "    \n",
    "    return len(unique_entities), unique_entities\n",
    "\n",
    "num_unique_entities, unique_entities = count_unique_entities(train_labels)\n",
    "\n",
    "print(\"Number of different entities:\", num_unique_entities)\n",
    "print(\"Unique entities:\", unique_entities)\n",
    "\n",
    "#Step 2\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(unique_entities))\n",
    "_train_labels = [label.split() for label in train_labels]\n",
    "train_numerical_labels = [label_encoder.transform(label) for label in _train_labels]\n",
    "train_pad_labels = pad_sequences(train_numerical_labels, maxlen=max_sequence_length, padding='post',value=119)\n",
    "\n",
    "def remove_sentences(list_labels, list_sequences):\n",
    "  idx_to_remove = []\n",
    "  labels_to_remove = []\n",
    "  for idx, labels in enumerate(list_labels):\n",
    "    for label in labels:\n",
    "      if label not in unique_entities:\n",
    "        idx_to_remove.append(idx)\n",
    "        labels_to_remove.append(label)\n",
    "\n",
    "  labels = [elem for i, elem in enumerate(list_labels) if i not in idx_to_remove]\n",
    "  sequences = [elem for i, elem in enumerate(list_sequences) if i not in idx_to_remove]\n",
    "  return labels, np.array(sequences)\n",
    "\n",
    "_test_labels = [label.split() for label in test_labels]\n",
    "test_labels_n, test_pad_sequences = remove_sentences(_test_labels, test_pad_sequences)\n",
    "test_numerical_labels = [label_encoder.transform(label) for label in test_labels_n]\n",
    "test_pad_labels = pad_sequences(test_numerical_labels, maxlen=max_sequence_length, padding='post',value=119) #119 es el index de pad\n",
    "\n",
    "_val_labels = [label.split() for label in val_labels]\n",
    "val_labels_n, val_pad_sequences = remove_sentences(_val_labels, val_pad_sequences)\n",
    "val_numerical_labels = [label_encoder.transform(label) for label in val_labels_n]\n",
    "val_pad_labels = pad_sequences(val_numerical_labels, maxlen=max_sequence_length, padding='post',value=119)\n",
    "\n",
    "\n",
    "#Step 3\n",
    "train_labels_one_hot = to_categorical(train_pad_labels, num_classes=num_unique_entities)\n",
    "test_labels_one_hot = to_categorical(test_pad_labels, num_classes=num_unique_entities)\n",
    "val_labels_one_hot = to_categorical(val_pad_labels, num_classes=num_unique_entities)\n",
    "\n",
    "print(train_pad_labels[0])\n",
    "\n",
    "print(train_labels_one_hot[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "975GjI0tp7_9"
   },
   "source": [
    "<h1><a name=\"section-three\"> 3. Disseny del model i entrenament </a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMY7LjpHv7GZ"
   },
   "source": [
    "---\n",
    "<h1><a name=\"ex-five\"><center> ‚úè Exercici 5 ‚úè</a></h1>\n",
    "\n",
    "De forma similar com f√©reu a la primera part de la pr√†ctica us demanem que dissenyeu l'arquitectura i entreneu el model. Podeu fer servir una arquitectura similar. Ara b√©, per capturar les depend√®ncies seq√ºencials podeu canviar la capa de GlobalMaxPooling1D per una LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1729339845913,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "O8Ed4x77t_4d",
    "outputId": "260aadd4-e692-46cc-e612-cbc4d43e6602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183510,) [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119]\n"
     ]
    }
   ],
   "source": [
    "s = train_pad_labels.shape\n",
    "train_flat_labels = train_pad_labels.reshape(s[0]*s[1])\n",
    "train_unq_labels = np.unique(train_flat_labels)\n",
    "print(train_flat_labels.shape, train_unq_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "XMPaZzX4sdH4"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):  #si cambiamos mida embbedings hay que cambiarlo aqui tmb\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"), #alomejor hacer bottleneck o al reves primero expandir y luego bottleneck\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1729339845913,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "PcWiSLm3nvdW",
    "outputId": "c78992d5-2225-4e73-ef9a-54793194fa1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veron\\Documents\\UNI\\3¬∫\\TVD\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m123/128\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3652 - loss: 4.0121"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 873\n'y' sizes: 897\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m     18\u001b[39m epochs = \u001b[32m30\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pad_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_pad_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels_one_hot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[32m     22\u001b[39m loss, accuracy = model.evaluate(test_pad_sequences, test_labels_one_hot,batch_size=batch_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veron\\Documents\\UNI\\3¬∫\\TVD\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veron\\Documents\\UNI\\3¬∫\\TVD\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:115\u001b[39m, in \u001b[36mcheck_data_cardinality\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m     sizes = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    112\u001b[39m         \u001b[38;5;28mstr\u001b[39m(i.shape[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree.flatten(single_data)\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    114\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 873\n'y' sizes: 897\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "\n",
    "embedding_dim = 32\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "#hacer bn \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, mask_zero=True))\n",
    "# model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=num_unique_entities, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "model.fit(train_pad_sequences, train_labels_one_hot, validation_data=(val_pad_sequences, val_labels_one_hot), batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_pad_sequences, test_labels_one_hot,batch_size=batch_size)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1729339845913,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "-ts_03xMaADp",
    "outputId": "8ca6b805-d682-43e6-98e7-a78ac14682ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1729339845913,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "4cP_IiZFmFK8",
    "outputId": "8fd62240-9089-4164-c836-53bc234a9834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885, 46, 120)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxFoy2UBktFS"
   },
   "outputs": [],
   "source": [
    "def preds_to_index(preds, seq_lens):\n",
    "  '''\n",
    "  Turn predictions to numerical indexes, flatten the sentences and discard padding.\n",
    "  '''\n",
    "  idx_preds = []\n",
    "  for pred, seq_len in zip(preds,seq_lens):\n",
    "      for l in range(seq_len):\n",
    "        idx_preds.append(np.argmax(pred[l]))\n",
    "  return idx_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeVIzSA4NYgD"
   },
   "source": [
    "Com ja sabeu, NER es una tasca on les dades estan molt desbalancejades. La gran majoria de les nostres etiquetes ser√°n 'O' (outside). Es pot donar el cas on el model tingui una accuracy molt alta predint sempre 'O'.\n",
    "\n",
    "Per mesurar millor com de b√≥ √©s el nostre model, calcularem la F1 score per cada classe, aix√≠ com la mitjana (macro average). Quan prepareu el vostre document, heu de reportar aquesta mitjana com a m√®trica del vostre model, no l'acuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1729339845913,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "f4DL7Tota13v",
    "outputId": "174eab75-10d9-4093-fbb3-6c88710cdc11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1549\n",
      "           1       1.00      0.79      0.88        24\n",
      "           2       0.92      0.83      0.87        29\n",
      "           3       0.94      0.95      0.94        79\n",
      "           4       1.00      0.12      0.22         8\n",
      "           5       1.00      0.00      0.00        18\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       1.00      0.00      0.00         7\n",
      "           8       1.00      0.00      0.00         3\n",
      "           9       1.00      0.00      0.00         3\n",
      "          13       1.00      0.00      0.00         2\n",
      "          14       1.00      0.00      0.00         3\n",
      "          15       0.73      0.62      0.67        13\n",
      "          16       0.58      0.79      0.67        14\n",
      "          17       0.64      0.63      0.64        43\n",
      "          18       0.80      1.00      0.89        24\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00        33\n",
      "          21       1.00      0.00      0.00         2\n",
      "          24       0.91      1.00      0.95        10\n",
      "          25       0.93      0.99      0.96       140\n",
      "          26       0.87      0.82      0.84        33\n",
      "          27       0.93      1.00      0.96        41\n",
      "          28       1.00      0.83      0.91         6\n",
      "          30       1.00      0.00      0.00         3\n",
      "          31       0.00      0.00      0.00         1\n",
      "          32       0.93      0.92      0.92        85\n",
      "          33       1.00      0.00      0.00         3\n",
      "          34       0.50      0.84      0.63        19\n",
      "          35       0.93      0.81      0.87        32\n",
      "          36       1.00      1.00      1.00         5\n",
      "          38       1.00      0.71      0.83        14\n",
      "          39       1.00      1.00      1.00         7\n",
      "          40       1.00      0.91      0.95        22\n",
      "          41       1.00      0.17      0.29         6\n",
      "          42       0.94      1.00      0.97        17\n",
      "          43       0.50      1.00      0.67         1\n",
      "          44       1.00      0.67      0.80         3\n",
      "          45       0.43      0.38      0.40         8\n",
      "          46       0.89      0.82      0.85       613\n",
      "          47       1.00      0.00      0.00        18\n",
      "          48       1.00      0.67      0.80        12\n",
      "          49       0.92      1.00      0.96        11\n",
      "          50       1.00      0.00      0.00         1\n",
      "          51       1.00      0.80      0.89         5\n",
      "          52       1.00      0.00      0.00         1\n",
      "          54       0.00      1.00      0.00         0\n",
      "          55       1.00      0.00      0.00         3\n",
      "          56       1.00      0.75      0.86         4\n",
      "          62       0.98      0.98      0.98        62\n",
      "          63       1.00      0.00      0.00         1\n",
      "          64       1.00      0.00      0.00         7\n",
      "          66       0.57      0.57      0.57         7\n",
      "          71       0.50      0.50      0.50         2\n",
      "          72       1.00      0.33      0.50         3\n",
      "          73       0.79      0.90      0.84       519\n",
      "          75       0.39      1.00      0.56        12\n",
      "          76       0.71      0.92      0.80        13\n",
      "          77       0.91      1.00      0.95        10\n",
      "          78       0.87      0.88      0.88        52\n",
      "          79       1.00      0.14      0.24        22\n",
      "          84       1.00      0.67      0.80         9\n",
      "          86       0.89      0.35      0.50        23\n",
      "          87       1.00      1.00      1.00        16\n",
      "          88       1.00      1.00      1.00         2\n",
      "          90       0.50      0.50      0.50         6\n",
      "          92       1.00      0.00      0.00         3\n",
      "          94       1.00      0.00      0.00         1\n",
      "          95       0.52      0.93      0.67        14\n",
      "          96       1.00      0.00      0.00         1\n",
      "         100       1.00      0.17      0.29         6\n",
      "         102       1.00      0.00      0.00         1\n",
      "         103       0.17      0.62      0.26         8\n",
      "         104       0.83      0.73      0.78       143\n",
      "         108       1.00      0.67      0.80         3\n",
      "         111       0.97      1.00      0.98        58\n",
      "         112       1.00      0.25      0.40         4\n",
      "         115       1.00      0.33      0.50         3\n",
      "         116       0.73      0.92      0.82       147\n",
      "         117       1.00      1.00      1.00         1\n",
      "         118       1.00      0.00      0.00         1\n",
      "         119       0.99      0.99      0.99      4865\n",
      "\n",
      "    accuracy                           0.94      9010\n",
      "   macro avg       0.87      0.56      0.55      9010\n",
      "weighted avg       0.95      0.94      0.94      9010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_labels_idx = preds_to_index(test_labels_one_hot,len_test_sequences)\n",
    "preds_idx = preds_to_index(preds, len_test_sequences)\n",
    "\n",
    "print(classification_report(test_labels_idx, preds_idx, zero_division=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lo que quiere es f1 score el macro y el nombre de las etiquetas, la o y padding deberia ser lo mismo y no deberian salir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VArxTPJgEaTx"
   },
   "source": [
    "A continuaci√≥ podeu veure algunes prediccions del model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1729339845913,
     "user": {
      "displayName": "Jordi Luque",
      "userId": "10816317814088701546"
     },
     "user_tz": -120
    },
    "id": "_S4H2EqYmiqs",
    "outputId": "4dd0bfcf-6a6e-4d3b-a537-bd5e02474cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Sentence:  i would like to find a flight from charlotte to las vegas that makes a stop in st. louis\n",
      "Original label:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'B-stoploc.city_name', 'I-stoploc.city_name']\n",
      "Predicted label:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'B-stoploc.city_name', 'I-stoploc.city_name', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  on april first i need a ticket from tacoma to san jose departing before 7 am\n",
      "Original label:  ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
      "Predicted label:  ['O', 'B-depart_date.month_name', 'B-class_type', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-arrive_time.time_relative', 'B-depart_time.time', 'I-depart_time.time', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  on april first i need a flight going from phoenix to san diego\n",
      "Original label:  ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
      "Predicted label:  ['O', 'B-depart_date.month_name', 'B-class_type', 'O', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  i would like a flight traveling one way from phoenix to san diego on april first\n",
      "Original label:  ['O', 'O', 'O', 'O', 'O', 'O', 'B-round_trip', 'I-round_trip', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number']\n",
      "Predicted label:  ['O', 'O', 'O', 'O', 'O', 'O', 'B-round_trip', 'I-round_trip', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-class_type', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  i would like a flight from orlando to salt lake city for april first on delta airlines\n",
      "Original label:  ['O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'B-airline_name', 'I-airline_name']\n",
      "Predicted label:  ['O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'B-airline_name', 'I-airline_name', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  i need a flight from toronto to newark one way leaving wednesday evening or thursday morning\n",
      "Original label:  ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-round_trip', 'I-round_trip', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day']\n",
      "Predicted label:  ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-round_trip', 'I-round_trip', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day', 'B-or', 'B-depart_date.day_name', 'B-depart_time.period_of_day', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  monday morning i would like to fly from columbus to indianapolis\n",
      "Original label:  ['B-depart_date.day_name', 'B-depart_time.period_of_day', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
      "Predicted label:  ['B-depart_date.day_name', 'B-depart_time.period_of_day', 'O', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-toloc.city_name', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  on wednesday april sixth i would like to fly from long beach to columbus after 3 pm\n",
      "Original label:  ['O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
      "Predicted label:  ['O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'I-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-toloc.city_name', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  after 12 pm on wednesday april sixth i would like to fly from long beach to columbus\n",
      "Original label:  ['B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
      "Predicted label:  ['B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-toloc.city_name', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sentence:  are there any flights from long beach to columbus on wednesday april sixth\n",
      "Original label:  ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number']\n",
      "Predicted label:  ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(test_pad_sequences)\n",
    "_predicted_labels = np.argmax(probs, axis=2)\n",
    "\n",
    "predicted_labels = [list(label_encoder.inverse_transform(label)) for label in _predicted_labels]\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print('Sentence: ', test_sentences[i])\n",
    "    print('Original label: ', test_labels[i])\n",
    "    print('Predicted label: ', predicted_labels[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65EQLymROmPX"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-six\"><center> ‚úè Exercici 6 ‚úè </a></h1>\n",
    "\n",
    "Modifiqueu els seg√ºents par√†metres del model anterior i analitzeu com afecten a la seva *accuracy*:\n",
    "\n",
    " 1. **Mida dels Embeddings.** Proveu diferents mides d'*Embeddings* i observeu com canvia l'*accuracy* del model. Heu d'explicar les vostres conclusions.\n",
    "\n",
    " 2. **Xarxes Convolucionals.** Afegiu capes convolucionals al vostre model. Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥ a l'hora d'escollir-los. Recordeu, que tamb√© podeu provar diferents configuracions de *pooling*.\n",
    "\n",
    " 3. **Xarxes Recurrents.**  Afegiu capes recurrents al vostre model (LSTM, GRU). Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥.\n",
    "\n",
    " 4. ** Transformer.** Afegiu blocs de Transformer al vostre model. Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥.\n",
    "\n",
    " 5. **Regularitzaci√≥.** Quan proveu configuracions amb m√©s par√†metres veureu que el model comen√ßa a tenir *overfitting* molt prompte durant l'entrenament. Afegiu *Dropout* al vostre model. Heu d'explicar la vostra decisi√≥ de valors i de posici√≥ dins de la xarxa.\n",
    "\n",
    "\n",
    " 6. **Balancejat de les classes.** Si analitzeu el dataset, veureu que la freq√º√®ncia de les classes est√† molt desbalancejada. Keras us permet afegir un pes per a cada classe a l'hora de calcular la loss (Mireu el par√†metre \"class_weigth\" a la documentaci√≥ https://keras.io/api/models/model_training_apis/). Calculeu un pes per a cada classe i afegiu-lo al m√®tode fit del vostre model.\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr2eNm7qJr4F"
   },
   "source": [
    "---\n",
    "\n",
    "<h1><a name=\"section-four\"> 4. Lliurable </a></h1>\n",
    "\n",
    "Heu d'entregar un document PDF de com a **m√†xim 10 p√†gines** que incloga els resultats de tots els exercicis aix√≠ com una explicaci√≥ de cadascun dels resultats i de la modificaci√≥ que heu fet. L'estructura del document √©s:\n",
    "\n",
    "1. Introducci√≥.\n",
    "2. Experiments i Resultats (amb raonament).\n",
    "3. Conclusions.\n",
    "\n",
    "No cal que afegiu el vostre codi al document, podeu entregar el *notebook* juntament amb el document.\n",
    "\n",
    " ---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
